# Ceph OSD 亲和性检查报告

**日期:** 2025-12-03
**集群命名空间:** poseidon
**检查节点:** dc1-stor40, dc1-stor41, dc1-stor42

## 执行摘要

对所有存储节点上的 OSD 进程亲和性与存储设备 NUMA 局部性进行了检查。

**检查结果:**
1.  **NUMA 拓扑:** 所有节点都有 2 个 NUMA 节点。
    -   **NUMA 0:** CPU 0, 2, ..., 70 (偶数核心)
    -   **NUMA 1:** CPU 1, 3, ..., 71 (奇数核心)
2.  **设备局部性:** 所有存储设备（用于 WAL/DB 的 NVMe 和用于数据的 HDD）都连接到 **NUMA 节点 0** 上的 PCI 控制器。
3.  **进程亲和性:** 所有 OSD 进程当前都以默认亲和性 (`0-71`) 运行，允许它们在 NUMA 节点 0 和 NUMA 节点 1 上调度。

**影响:**
当前配置允许 OSD 进程在 NUMA 节点 1 上运行，同时访问 NUMA 节点 0 上的硬件。这种跨 NUMA 访问会引入额外的延迟和 QPI/UPI 互连流量，可能会降低性能，特别是对于高速 NVMe 设备。

**建议:**
将所有 OSD 进程绑定到 **NUMA 节点 0** 的 CPU，以确保本地内存和 I/O 访问。

## 详细结果

### 节点: dc1-stor40

| 设备类别 | OSD ID | 物理设备 | 设备 NUMA | 当前亲和性 | 建议亲和性 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| NVMe | 0, 3, 6 | /dev/nvme3n1 | 0 | 0-71 (全部) | NUMA 0 (0,2...70) |
| HDD | 9, 12, 15, 19, 22, 25, 28, 31, 34, 37, 41, 44 | /dev/sd[a-z]+ | 0 | 0-71 (全部) | NUMA 0 (0,2...70) |

### 节点: dc1-stor41

| 设备类别 | OSD ID | 物理设备 | 设备 NUMA | 当前亲和性 | 建议亲和性 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| NVMe | 1, 4, 7 | /dev/nvme3n1 | 0 | 0-71 (全部) | NUMA 0 (0,2...70) |
| HDD | 10, 13, 16, 18, 21, 24, 27, 30, 33, 36, 40, 43 | /dev/sd[a-z]+ | 0 | 0-71 (全部) | NUMA 0 (0,2...70) |

### 节点: dc1-stor42

| 设备类别 | OSD ID | 物理设备 | 设备 NUMA | 当前亲和性 | 建议亲和性 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| NVMe | 2, 5, 8 | /dev/nvme1n1 | 0 | 0-71 (全部) | NUMA 0 (0,2...70) |
| HDD | 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 39, 42 | /dev/sd[a-z]+ | 0 | 0-71 (全部) | NUMA 0 (0,2...70) |

## 实施计划

要应用此优化，需要更新 OSD 配置（例如，通过 Rook 配置或 systemd 覆盖（如果适用），但通常通过 Rook 的 `resources` 或特定的亲和性设置（如果支持），或在容器上使用 `cpuset`）。

由于 Rook 管理这些资源，添加 `nodeAffinity` 或 `topologySpreadConstraints` 有助于 Pod 放置，但对于节点内的 CPU 绑定，通常需要设置：
-   **Rook/Ceph 配置:** `ceph config set global osd_numa_node 0` （验证这是否适用于所有 OSD）。
-   **Kubernetes CPU 管理器:** 如果使用静态策略，请确保容器获得保证并落在正确的 NUMA 节点上。
-   **手动/脚本:** 使用 `taskset` 或 `systemd` 钩子（在 K8s 中持久性较差）。

*注意：由于所有设备都在 NUMA 0 上，简单地将这些节点上的所有 OSD Pod 限制在 NUMA 0 的 CPU 上是正确的做法。*
