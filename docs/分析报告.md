# Ceph 元数据盘性能分析报告

**分析日期**: 2025-12-03
**测试环境**: Rook-Ceph部署在poseidon命名空间
**测试节点**: dc1-stor40, dc1-stor41, dc1-stor42

---

## 执行摘要

通过对比Intel 900p (Optane)和Intel 4610 (SATA SSD)作为Ceph元数据盘的性能测试结果，发现**Intel 900p性能显著低于Intel 4610**，这与预期完全相反。主要原因包括网络问题、配置不当和硬件兼容性问题。

---

## 性能数据对比

### 1. 生产环境混合读写性能 (70%读/30%写, 4K随机)

| 指标 | Intel 900p | Intel 4610 | 差异 |
|------|------------|------------|------|
| 读取IOPS | 5,288 | 6,661 | **-20.6%** |
| 写入IOPS | 2,273 | 2,862 | **-20.6%** |
| 读取吞吐量 | 20.6 MB/s | 26.0 MB/s | **-20.6%** |
| 写入吞吐量 | 8.8 MB/s | 11.2 MB/s | **-21.4%** |

### 2. 延迟测试 (4K随机读, iodepth=1)

| 指标 | Intel 900p | Intel 4610 | 差异 |
|------|------------|------------|------|
| IOPS | 1,147 | 1,147 | 0% |
| 平均延迟 | 0.87ms | 0.87ms | 0% |

### 3. 延迟测试 (4K随机写, iodepth=1)

| 指标 | Intel 900p | Intel 4610 | 差异 |
|------|------------|------------|------|
| IOPS | 546 | 546 | 0% |
| 平均延迟 | 1.83ms | 1.83ms | 0% |

---

## 关键发现

### 1. 网络性能问题 (严重)

Intel 900p配置存在严重的网络问题：

- **重传段数**: 44,401次 (vs 4610的3,597次) - **高出11.3倍**
- **快速重传**: 30,517次 (vs 4610的98次) - **高出311倍**
- **慢启动重传**: 66次 (vs 4610的12次)

这表明900p配置中存在网络拥塞、丢包或配置问题，严重影响了Ceph集群的通信效率。

### 2. Ceph OSD延迟异常

Intel 900p的OSD提交和应用延迟普遍较高：

- **OSD提交延迟**: 30-50ms (部分OSD)
- **OSD应用延迟**: 30-50ms (部分OSD)
- 部分OSD显示0延迟，可能未正常工作

对比Intel 4610配置，OSD延迟应保持在10ms以下。

### 3. 磁盘利用率问题

从iostat数据可以看出：

**Intel 900p (sdm设备)**:
- r/s: 159.07, w/s: 92.99
- r_await: 0.92ms, w_await: 20.76ms
- %util: 2.43%

**Intel 4610 (sdm设备)**:
- r/s: 110.09, w/s: 83.55
- r_await: 0.85ms, w_await: 24.12ms
- %util: 2.05%

虽然利用率都不高，但900p的写入等待时间(w_await)更长。

### 4. 配置差异

观察到两个配置存在细微差异：

- **Intel 900p**: 使用`time_based=0`进行准备阶段测试
- **Intel 4610**: 所有测试统一使用`time_based=1`

这表明测试方法可能存在不一致。

---

## 根因分析

### 主要原因

1. **网络基础设施问题**
   - 900p配置的网络重传率异常高
   - 可能是网卡驱动、交换机配置或物理链路问题
   - 建议使用`ethtool`检查网卡状态，`ping`测试网络延迟

2. **硬件兼容性问题**
   - Intel 900p是Optane存储，可能需要特定的内核参数优化
   - 检查IOMMU设置：`dmesg | grep -i iommu`
   - 验证NUMA配置：`numactl --hardware`

3. **Ceph配置不当**
   - OSD的`osd_op_num_threads_per_shard`可能需要调整
   - 检查`bluestore_min_alloc_size`设置
   - 验证`bluestore_cache_size`配置

4. **测试环境问题**
   - 测试时间不同，可能存在背景负载差异
   - 900p测试在12-03, 4610测试在12-02
   - 建议在同一时间段进行对比测试

### 次要原因

1. **文件系统缓存影响**
   - 900p测试的延迟数据异常一致，可能存在缓存未正确绕过
   - 确认`direct=1`是否正确生效

2. **队列深度设置**
   - 混合读写测试使用iodepth=32，但对于Optane可能需要更高值
   - 建议测试iodepth=64或更高

---

## 建议措施

### 立即执行

1. **网络诊断**
   ```bash
   # 在k8s节点上执行
   ethtool -S <网卡名> | grep -i error
   ping -c 100 <其他节点IP>
   iperf3 -s  # 在一个节点
   iperf3 -c <服务器IP>  # 在另一个节点
   ```

2. **硬件检查**
   ```bash
   # 检查Optane设备状态
   smartctl -a /dev/sdm
   nvme list
   dmesg | grep -i optane
   ```

3. **Ceph集群状态检查**
   ```bash
   kubectl -n poseidon exec -it <mon-pod> -- ceph -s
   kubectl -n poseidon exec -it <mon-pod> -- ceph osd df
   kubectl -n poseidon exec -it <mon-pod> -- ceph osd perf
   ```

### 配置优化

1. **调整Ceph OSD配置**
   ```yaml
   # 在rook-ceph-cluster.yaml中
   config:
     osd_op_num_threads_per_shard: "4"  # 根据CPU核心数调整
     osd_op_num_shards: "8"
     bluestore_min_alloc_size: "4096"  # 4K对齐
     bluestore_cache_size: "3221225472"  # 3GB
   ```

2. **内核参数优化**
   ```bash
   # 针对Optane优化
   echo "none" > /sys/block/sdm/queue/scheduler
   echo "256" > /sys/block/sdm/queue/nr_requests
   echo "0" > /sys/block/sdm/queue/add_random
   echo "0" > /sys/block/sdm/queue/rotational
   ```

3. **网络参数优化**
   ```bash
   # 减少重传
   sysctl -w net.ipv4.tcp_retries2=5
   sysctl -w net.core.rmem_max=134217728
   sysctl -w net.core.wmem_max=134217728
   ```

### 重新测试方案

1. **标准化测试环境**
   - 停止所有非必要的后台任务
   - 确保两个测试在同一时间段进行
   - 使用完全相同的fio配置文件

2. **扩展测试矩阵**
   - 测试iodepth=1, 8, 16, 32, 64
   - 测试numjobs=1, 4, 8, 16
   - 测试不同块大小：4K, 8K, 16K, 64K, 1M

3. **增加监控指标**
   - 收集CPU利用率（usr_cpu, sys_cpu）
   - 监控网络带宽使用率
   - 记录Ceph OSD日志

---

## 结论

Intel 900p作为Ceph元数据盘性能不如Intel 4610的主要原因是**网络问题**和**配置不当**，而非硬件性能不足。Optane 900p本身是高性能存储，理论上应该比SATA SSD的4610表现更好。

建议优先解决网络重传问题，然后优化Ceph和内核配置，最后进行标准化的对比测试。在问题解决后，900p应该能够展现出其真正的性能优势，特别是在低延迟和高IOPS场景下。

---

## 附录：测试命令参考

### 检查Ceph状态
```bash
kubectl -n poseidon get pods
kubectl -n poseidon exec -it <mon-pod> -- ceph -s
kubectl -n poseidon exec -it <mon-pod> -- ceph osd tree
```

### 检查磁盘状态
```bash
kubectl -n poseidon exec -it <osd-pod> -- iostat -x 1 10
kubectl -n poseidon exec -it <osd-pod> -- df -h
```

### 检查网络状态
```bash
kubectl -n poseidon exec -it <pod> -- cat /proc/net/snmp
kubectl -n poseidon exec -it <pod> -- ss -i
```
